<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://anuraggithub.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://anuraggithub.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-05-12T10:20:31+00:00</updated><id>https://anuraggithub.github.io/feed.xml</id><title type="html">blank</title><subtitle>Welcome to my personal blog! I am a master student in Data Science at RWTH-Aachen with a focus on deep learning. My interests lie in the intersection of Vision, Language and general Machine Learning. My goal is to share my research findings, thoughts and experiences on the latest developments in the field of machine learning and its applications in computer vision and natural language processing. \n As a researcher, I have been working on various Machine Learning projects and have a strong working knowledge of the field. I have industry experience of more than 3 years in development and deployment of statistical machine learning models in production with end-to-end software development cycle. My current research focuses on developing new techniques for improving the performance of vision and language models. I am also interested in exploring the generalization capabilities of these models and developing methods to improve their robustness.\n On this blog, you will find a variety of posts on topics such as natural language processing, computer vision, deep learning, and more. I will also be sharing my thoughts on the latest research papers and the direction of the field. My hope is that my blog will be a valuable resource for anyone interested in Machine Learning, and I welcome any feedback or questions you may have. </subtitle><entry><title type="html">The new vision langauge affair</title><link href="https://anuraggithub.github.io/blog/2023/Vision-langauge-start/" rel="alternate" type="text/html" title="The new vision langauge affair"/><published>2023-01-23T16:40:16+00:00</published><updated>2023-01-23T16:40:16+00:00</updated><id>https://anuraggithub.github.io/blog/2023/Vision-langauge-start</id><content type="html" xml:base="https://anuraggithub.github.io/blog/2023/Vision-langauge-start/"><![CDATA[<p>Understanding modalities first!</p> <p>Computer vision and language modelling co-existed and covered the major percentage of machine learning research ever since the burst of feasible and accessible computation. Tasks involving multiple modalities have been studied/researched for years, but the generalization capabilities of transformer based models have opened a new paradigm to venture to. Modalities in literal sense refers to the way information is represented. Therefore, multi-modal means a setup having data of different modalities or representations in their crude form. These modalities can be image, text, audio, video etc. There’s another term widely used in such context which is cross-modal learning or paradigm, which means learning via utlizing different modalities to understand/generation knowledge representation of the target objective. Multi-modality doesn’t necessarily means that all modalities will be exploited to understand the scene/target better.</p> <p>Pre-training/transfer learning in a nutshell!</p> <p>In order to achieve strong results or performance using deep learning, usually it takes a vast amount of data related to the task at hand with significant computation capability. Furthermore, general machine learning models have the underlying assumption that the distribution of training and test data is similar in the feature space. These assumptions are often not valid in real-world problems and hence this poses an impending challenge. Transfer learning or knowledge transfer allows for tackling these challenges. The very traditional work and motivation for the use of transfer learning methodology date back to 19951. Over the past decade, or so has seen many different evolution and forms of this technique. Even with varied names and forms, the core essence of transfer learning is to utilize the knowledge obtained from an initial task or source task and apply it to improve the performance of the learning to a different task or be a target task.</p> <hr/> <p>So, the obvious question here is why to use different modalities and what advantage do we get by doing so. This can be answered by mix of techniques and solutions. Depending upon task at hand, using multi-modal and cross-modal can help in achieving better results as shown in recent research in the field. To categorize these to understand well, we use certain aspects to split these models as mentioned in recent paper by Chen et. al 2022.</p> <h3 id="key-aspects-to-split-the-model-types">Key aspects to split the model types:</h3> <ul> <li>Model architecture</li> <li>Pre-training objectives</li> <li>Downstream tasks</li> </ul> <p>Here, we briefly describe these aspects which will be then explored in details. Also, keep in mind there can be overlap between these aspects as models would fit in more than one category.</p> <p>Model architectures: How one trains the different modal models determines the architecture of VLP models. This also, overlaps with terms discussed before of multi-modal and cross-modal models. Either the model can be single stream i.e takes the feature representations from both the vision and language model and concates them onto the rest of the network, or trains them separately while having cross sharing of features using something like cross-attention. This will be discussed in detail in upcoming blog.</p> <p>Pre-training objectives: What representations are learnt via network conjointly is primarily determined by the loss function or learning objective. Some of the main ones known in vision-langauge paradigm are as follows: Completion : Masked image or language modelling in which certain portions of data are attempted to reconstruct given some unmasked ones. Matching : Collectively using both signals to understand the task at hand better. Temporal : Distruptions are added by re-ordering certain sequences to make models robust. Special cases: Visual question answering and captioning objectives.</p> <p>Downstream tasks: More often than not, downstream tasks determine the model type. Certain models are trained in two step process. The key terms here are Upstream components which are the ones involved during the pre-training phase and downstream components are those involved in fine-tuning for specific tasks. For example, you train a vision langauge model to obtain certain feature representations and then use these to perform specific tasks like visual question answering etc.</p> <p>These categories will be discussed in depth in next ones.</p> <hr/>]]></content><author><name></name></author><category term="sample-posts"/><category term="computer"/><category term="vision,"/><category term="language"/><category term="models"/><summary type="html"><![CDATA[Closer look at various vision-langauge pre-training models (specifically VLP)]]></summary></entry><entry><title type="html">5-Minute Papers :- PaLI</title><link href="https://anuraggithub.github.io/blog/2023/5min-paper-PaLI/" rel="alternate" type="text/html" title="5-Minute Papers :- PaLI"/><published>2023-01-23T16:40:16+00:00</published><updated>2023-01-23T16:40:16+00:00</updated><id>https://anuraggithub.github.io/blog/2023/5min-paper-PaLI</id><content type="html" xml:base="https://anuraggithub.github.io/blog/2023/5min-paper-PaLI/"><![CDATA[<h3 id="tldr">TL;DR</h3> <p>PaLI(Pathways Langauge and Image Model) introduces as class of vision-language models that are scalable and flexible in its interface. Flexibility depicts the ability of the model to perform various types of downstream tasks like image captioning, visual question answering, and scene-text understanding. It outperforms existing SOTAs even with being trained with lesser number of parameters, and excels at many tasks even in multi-lingual setups. New Additions:</p> <ul> <li>Largest ViT model till date i.e. ViT-e</li> <li>Multi-lingual support</li> <li>Mix of pre-training tasks enabling model with wide range of downstream capabilities.</li> <li>Scalable and re-usable modular components.</li> <li>Mix of single and dual stream architecture.</li> </ul> <hr/> <h4 id="architecture">Architecture</h4> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pali_arch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pali_arch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pali_arch-1400.webp"/> <img src="/assets/img/pali_arch.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Major Components:</p> <ul> <li>ViT for image encoding (ViT-e)</li> <li>Text Encoder-Decoder transformers architecture (pre-trained mT5)</li> <li>No task based head, but prompts are used to differentiate between the tasks</li> </ul> <hr/> <h4 id="pre-training-and-fine-tuning-process">Pre-training and Fine-tuning process</h4> <p>For both the pre-training and fine-tuning, same model is used. The model accepts input as an image and a text string, generated text string as outputs. As mentioned before, there are task specific heads used in the model, but instead text-based prompts are used to indicate the type of task at hand.</p> <hr/> <h4 id="prompt-based-tasks-mixture">Prompt-based tasks mixture</h4> <p>To incorporate diverse tasks, the text-based prompts corresponding to different tasks is used. Examples of few are listed, more exhaustive description can be found be in the original <a href="https://arxiv.org/pdf/2209.06794v2.pdf">paper</a>.</p> <ul> <li> <p>Captioning task SplitCap on WebLI alt-text data Prompt: “Generate the alt-text in <lang> at <pos>: <cap1> <extra_id_0> Target: <cap2></cap2></extra_id_0></cap1></pos></lang></p> </li> <li> <p>English and Cross-Lingual VQA Triplets =&gt; &lt;Image, [question], [answer]&gt; Prompt: “Answer in EN: [question] <extra_id_0i>" Target: [answer]</extra_id_0i></p> </li> </ul> <hr/> <h4 id="experiments-and-results">Experiments and Results</h4> <p>As per the paper, PaLI outperforms the current SOTAs across various benchmarks in various downstream tasks. Summary of performance in Image captioning and VQA is displayed below:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pali_results-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pali_results-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pali_results-1400.webp"/> <img src="/assets/img/pali_results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="references">References</h4> <p>Original Paper: <a href="https://arxiv.org/pdf/2209.06794v2.pdf">arxiv</a> Google blog: <a href="https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html">blog</a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="computer"/><category term="vision,"/><category term="language"/><category term="models"/><summary type="html"><![CDATA[PaLI by google-research is a jointly-scaled multilingual Language-Image Model]]></summary></entry><entry><title type="html">Gradient Descent through the mathematical prism</title><link href="https://anuraggithub.github.io/blog/2019/gradient-descent-through-the-mathematical-prism/" rel="alternate" type="text/html" title="Gradient Descent through the mathematical prism"/><published>2019-09-14T16:17:54+00:00</published><updated>2019-09-14T16:17:54+00:00</updated><id>https://anuraggithub.github.io/blog/2019/gradient-descent-through-the-mathematical-prism</id><content type="html" xml:base="https://anuraggithub.github.io/blog/2019/gradient-descent-through-the-mathematical-prism/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Demystifying Markov Clustering</title><link href="https://anuraggithub.github.io/blog/2019/demystifying-markov-clustering/" rel="alternate" type="text/html" title="Demystifying Markov Clustering"/><published>2019-08-25T16:10:31+00:00</published><updated>2019-08-25T16:10:31+00:00</updated><id>https://anuraggithub.github.io/blog/2019/demystifying-markov-clustering</id><content type="html" xml:base="https://anuraggithub.github.io/blog/2019/demystifying-markov-clustering/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Getting more out of text data…Text to Information Part 1</title><link href="https://anuraggithub.github.io/blog/2019/getting-more-out-of-text-datatext-to-information-part-1/" rel="alternate" type="text/html" title="Getting more out of text data…Text to Information Part 1"/><published>2019-08-16T20:03:26+00:00</published><updated>2019-08-16T20:03:26+00:00</updated><id>https://anuraggithub.github.io/blog/2019/getting-more-out-of-text-datatext-to-information-part-1</id><content type="html" xml:base="https://anuraggithub.github.io/blog/2019/getting-more-out-of-text-datatext-to-information-part-1/"><![CDATA[]]></content><author><name></name></author></entry></feed>