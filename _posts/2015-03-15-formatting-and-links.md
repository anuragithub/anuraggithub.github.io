---
layout: post
title:  The new vision langauge affair
date:   2023-01-23 16:40:16
description: Closer look at various vision-langauge pre-training models (specifically VLP)
tags: computer vision, language models
categories: sample-posts
---
Understanding modalities first!

Computer vision and language modelling co-existed and covered the major percentage of machine learning research ever since the burst of feasible and accessible computation. Tasks involving multiple modalities have been studied/researched for years, but the generalization capabilities of transformer based models have opened a new paradigm to venture to.
Modalities in literal sense refers to the way information is represented. Therefore, multi-modal means a setup having data of different modalities or representations in their crude form. These modalities can be image, text, audio, video etc. There's another term widely used in such context which is cross-modal learning or paradigm, which means learning via utlizing different modalities to understand/generation knowledge representation of the target objective. Multi-modality doesn't necessarily means that all modalities will be exploited to understand the scene/target better.


Pre-training/transfer learning in a nutshell!

In order to achieve strong results or performance using deep learning, usually it takes a vast amount of data related to the task at hand with significant computation capability. Furthermore, general machine learning models have the underlying assumption that the distribution of training and test data is similar in the feature space. These assumptions are often not valid in real-world problems and hence this poses an impending
challenge. Transfer learning or knowledge transfer allows for tackling these challenges. The very traditional work and motivation for the use of transfer learning methodology date back to 19951. Over the past decade, or so has seen many different evolution and forms of this technique. Even with varied names and forms, the core essence of transfer learning is to utilize the knowledge obtained from an initial task or source task and apply it to improve the performance of the learning to a different task
or be a target task.
#### Hipster list
<ul>
    <li>brunch</li>
    <li>fixie</li>
    <li>raybans</li>
    <li>messenger bag</li>
</ul>

Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90's yr typewriter selfies letterpress cardigan vegan.

<hr>

Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.

<blockquote>
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    â€”Anais Nin
</blockquote>

Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.
