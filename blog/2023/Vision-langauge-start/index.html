<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The new vision langauge affair | Anurag Kumar Mishra</title> <meta name="author" content="Anurag Kumar Mishra"> <meta name="description" content="Closer look at various vision-langauge pre-training models (specifically VLP)"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://anuraggithub.github.io/blog/2023/Vision-langauge-start/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Anurag </span>Kumar Mishra</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">The new vision langauge affair</h1> <p class="post-meta">January 23, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/computer"> <i class="fas fa-hashtag fa-sm"></i> computer</a>   <a href="/blog/tag/vision"> <i class="fas fa-hashtag fa-sm"></i> vision,</a>   <a href="/blog/tag/language"> <i class="fas fa-hashtag fa-sm"></i> language</a>   <a href="/blog/tag/models"> <i class="fas fa-hashtag fa-sm"></i> models</a>     ·   <a href="/blog/category/sample-posts"> <i class="fas fa-tag fa-sm"></i> sample-posts</a>   </p> </header> <article class="post-content"> <p>Understanding modalities first!</p> <p>Computer vision and language modelling co-existed and covered the major percentage of machine learning research ever since the burst of feasible and accessible computation. Tasks involving multiple modalities have been studied/researched for years, but the generalization capabilities of transformer based models have opened a new paradigm to venture to. Modalities in literal sense refers to the way information is represented. Therefore, multi-modal means a setup having data of different modalities or representations in their crude form. These modalities can be image, text, audio, video etc. There’s another term widely used in such context which is cross-modal learning or paradigm, which means learning via utlizing different modalities to understand/generation knowledge representation of the target objective. Multi-modality doesn’t necessarily means that all modalities will be exploited to understand the scene/target better.</p> <p>Pre-training/transfer learning in a nutshell!</p> <p>In order to achieve strong results or performance using deep learning, usually it takes a vast amount of data related to the task at hand with significant computation capability. Furthermore, general machine learning models have the underlying assumption that the distribution of training and test data is similar in the feature space. These assumptions are often not valid in real-world problems and hence this poses an impending challenge. Transfer learning or knowledge transfer allows for tackling these challenges. The very traditional work and motivation for the use of transfer learning methodology date back to 19951. Over the past decade, or so has seen many different evolution and forms of this technique. Even with varied names and forms, the core essence of transfer learning is to utilize the knowledge obtained from an initial task or source task and apply it to improve the performance of the learning to a different task or be a target task.</p> <hr> <p>So, the obvious question here is why to use different modalities and what advantage do we get by doing so. This can be answered by mix of techniques and solutions. Depending upon task at hand, using multi-modal and cross-modal can help in achieving better results as shown in recent research in the field. To categorize these to understand well, we use certain aspects to split these models as mentioned in recent paper by Chen et. al 2022.</p> <h3 id="key-aspects-to-split-the-model-types">Key aspects to split the model types:</h3> <ul> <li>Model architecture</li> <li>Pre-training objectives</li> <li>Downstream tasks</li> </ul> <p>Here, we briefly describe these aspects which will be then explored in details. Also, keep in mind there can be overlap between these aspects as models would fit in more than one category.</p> <p>Model architectures: How one trains the different modal models determines the architecture of VLP models. This also, overlaps with terms discussed before of multi-modal and cross-modal models. Either the model can be single stream i.e takes the feature representations from both the vision and language model and concates them onto the rest of the network, or trains them separately while having cross sharing of features using something like cross-attention. This will be discussed in detail in upcoming blog.</p> <p>Pre-training objectives: What representations are learnt via network conjointly is primarily determined by the loss function or learning objective. Some of the main ones known in vision-langauge paradigm are as follows: Completion : Masked image or language modelling in which certain portions of data are attempted to reconstruct given some unmasked ones. Matching : Collectively using both signals to understand the task at hand better. Temporal : Distruptions are added by re-ordering certain sequences to make models robust. Special cases: Visual question answering and captioning objectives.</p> <p>Downstream tasks: More often than not, downstream tasks determine the model type. Certain models are trained in two step process. The key terms here are Upstream components which are the ones involved during the pre-training phase and downstream components are those involved in fine-tuning for specific tasks. For example, you train a vision langauge model to obtain certain feature representations and then use these to perform specific tasks like visual question answering etc.</p> <p>These categories will be discussed in depth in next ones.</p> <hr> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Anurag Kumar Mishra. Thank you for visiting my blog! Please feel free to leave comments or contact me with any questions or thoughts about my posts. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>